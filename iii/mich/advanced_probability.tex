\documentclass{article}

\def\npart{III}
\def\nyear{2019}
\def\nterm{Michaelmas}
\def\nlecturer{Dr. S. Andres}
\def\ncourse{Advanced Probability}
\def\draft{Incomplete}

% preamble
\usepackage{imakeidx}
\usepackage{marginnote}
\usepackage{chngcntr}
\usepackage{bbm}
\usepackage{xfrac}

\input{header}
\makeindex[intoc]
\reversemarginpar

\newtheorem{nremark}[nthm]{Remark}

\newcommand{\named}[1]{\textbf{#1}\index{#1}}

\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

% and here we go!

\begin{document}
\maketitle

\tableofcontents

\clearpage
\newlec
\section{Conditional Expectations}
\newlec
Take a \named{probability space} $(\Omega, \mathcal{F}, \mathbb{P})$, meaning $\mathcal{F}$ is a $\sigma$-algebra and $\mathbb{P}$ is a probability measure, with $\mathbb{P}(\Omega) = 1$.
\hypertarget{def:as}We use the term `\named{almost surely}' (or a.s.) to mean almost everywhere.

\hypertarget{def:e}Take $X$ to be a random variable, i.e.\ $X: \Omega \to \mathbb{R}$ which is $\mathcal{F}$-measurable and write
\begin{equation*}
  \mathbb{E}[X] = \int X\, d\mathbb{P}
\end{equation*}
for the \named{expectation} of $X$.
We write also
\begin{equation*}
  \mathbb{E}[X \1A] = \int_A X \, d\mathbb{P}
\end{equation*}
for $A \in \mathcal{F}$.
\begin{ndef}
  Let $B \in \mathcal{F}$ with $\mathbb{P}[B] > 0$. We know
  \begin{equation*}
    \mathbb{P}[A | B] = \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]},
  \end{equation*}
  the \named{conditional probability} of $A$ given $B$.
  Similarly,
  \begin{equation*}
    \mathbb{E}[X | B] = \frac{\mathbb{E}[X \1B]}{\mathbb{P}[B]}
  \end{equation*}
  the \named{conditional expectation} of $X$ given $B$.
\end{ndef}
There is a significant restriction to this definition: that $\mathbb{P}[B] > 0$. By the end of this lecture, we will generalise this definition to any $\sigma$-algebra of events, rather than just one.

\begin{aim}
  Improve the prediction of $X$ if additional information (given as a sub-$\sigma$-algebra $\mathcal{G} \subseteq \mathcal{F}$) is available.
\end{aim}

\subsection{Discrete case}
Take $B_1, B_2, \dotsc \in \mathcal{F}$ a disjoint decomposition of $\Omega$.
We take
\begin{equation*}
  \mathcal{G} = \sigma(B_1, B_2, \dotsc) = \left\{\bigcup_{i \in J} B_i : J \subseteq \mathbb{N}\right\} \subseteq \F.
\end{equation*}
That is, the `extra information' of $\G$ is that we know which of the disjoint events $B_i$ we fall into.

Then,
\begin{equation*}
  \E[X | \G] (\omega) \coloneqq \sum_{i : \Prob[B_i] > 0} \E[X | B_i] \1{B_i}(\omega)
\end{equation*}
is the conditional expectation of $X$ given $\G$.

It is easy to see that $\E[X | \G]$ is a $\G$-measurable random variable, and
\begin{equation*}
  \E[\1{A} X] = \E[\1{A} \E[X|\G]] \quad \forall A \in \G
.\end{equation*}

\begin{eg}\leavevmode
  \begin{enumerate}[(i)]
    \item
      Take now $\Omega = (0, 1]$, and $\F = \mathcal{B} (\Omega)$, and $\Prob$ to be Lebesgue measure.
      Use $X$ as shown below, and use
      \begin{equation*}
        \G = \sigma\left(\left(\tfrac{k}{m}, \tfrac{k+1}m\right] : k = 0, \dotsc, m-1\right)
      .\end{equation*}
      In the picture, we take $m=8$, and the conditional expectation $\E(X | \G)$ is shown.
      \begin{center}
        \begin{tikzpicture}[scale=5]
          \draw [->] (0,0) -- (0,0.7);
          \draw [->] (0,0) -- (1.1,0);
          \draw plot [smooth, tension=1] coordinates {(0,0.4) (0.3,0.6) (0.6,0.2) (1,0.3)};
          \foreach \x in {0.125,0.25,...,1} {
            \draw (\x,-0.02) -- (\x,0.02);
          }
          \node at (-0.07, 0.7) {$X$};
          \node at (1, -0.07) {$1$};
          \node at (1.14, -0.07) {$\Omega$};
          \draw (0,0.49) -- (0.125,0.49);
          \draw (0.125,0.59) -- (0.25,0.59);
          \draw (0.25,0.585) -- (0.375,0.585);
          \draw (0.375,0.42) -- (0.5,0.42);
          \draw (0.5,0.225) -- (0.625,0.225);
          \draw (0.625,0.182) -- (0.75,0.182);
          \draw (0.75,0.195) -- (0.875,0.195);
          \draw (0.875,0.255) -- (1,0.255);
        \end{tikzpicture}
      \end{center}
    \item Take a random variable $Z: \Omega \to \{z_1, z_2, \dotsc\} \subseteq \mathbb{R}$, and use $\G = \sigma(Z) = \sigma(\{Z = z_i\} : i = 1,2, \dotsc)$.
      Then,
      \begin{align*}
        \E[X | Z]  &\coloneqq \E[X | \sigma(Z)]  \\
      &= \sum_{{i : \Prob[Z = z_i] > 0}} \E[X | Z=z_i] \1{\{Z=z_i\}}
      .\end{align*}
  \end{enumerate}
\end{eg}
This is not satisfactory quite yet: if $Z$ has an absolutely continuous distribution (eg $\mathcal{N}(0,1)$), i.e.\ $\mathcal{P}[Z =z] = 0$ for every $z$, then $\E[X | Z]$ is not defined yet!

\subsection{General case}
\begin{ndef}\label{def:1.2}
  \hypertarget{def:eg}Let $X \in L^1 (\Omega, \F, \Prob)$, $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. A random variable $Y$ is called (a version of) the \named{conditional expectation} of $X$ given $\G$ if
  \begin{enumerate}[(i)]
    \item $Y$ is $\G$-measurable
    \item $\E[X \1A] = \E[Y \1A]$ for all $A \in \G$.
  \end{enumerate}
  We notate $Y = \E[X | \G]$.
\end{ndef}
\begin{nremark}\leavevmode
  \begin{enumerate}[(a)]
    \item We took $X \in L^1$, but this can be changed to $X \geq 0$ throughout.
    \item If $\G = \sigma(\mathcal{C})$ for some $\mathcal{C} \subseteq \F$ which is a \hypertarget{def:pisystem}{\textbf{$\pi$-system}}\index{$\pi$-system@pi system} (i.e. stable under intersections), it suffices to check (ii) for all $A \in \mathcal{C}$.
    \item If $\G = \sigma(Z)$ where $Z$ is a random variable, we write $\E[X | Z] \coloneqq \hyperlink{def:eg}{\E[X | \sigma(Z)]}$. This is $\sigma(Z)$ measurable by (i), so it's of the form $f(Z)$ for some function $f$.
      It's then common to define $\E[X | Z=z] = f(z)$.
  \end{enumerate}
\end{nremark}
\begin{nthm}[Existence and uniqueness]\label{thm:1.4}
  Let $X \in L^1(\Omega, \F, \Prob)$, and $\G \subseteq \F$ a $\sigma$-algebra.
  \begin{enumerate}[(i)]
    \item $\hyperlink{def:eg}{E[X | \G]}$ exists
    \item Any two versions of $\E[X | \G]$ coincide $\Prob$-almost surely.
  \end{enumerate}
\end{nthm}
\begin{proof}\leavevmode
  \begin{enumerate}[(i)]
    \item[(ii)] Uniqueness. Let $Y$ be as in \cref{def:1.2}, and let $Y'$ satisfy \cref*{def:1.2}(i) and (ii) for some $X' \in L^1$ with $X \leq X'$ almost surely.
      Let $Z = (Y-Y') \1A$ with $A \coloneqq \{Y \geq Y'\} \in \G$.
      \begin{equation*}
        \E[Y \1A] = % def 1.2 ii)
        E[X \1A] \leq % X \leq X' a.s.
        \E[X' \1A] =
        \E[Y' \1A] < \infty
      \end{equation*}
      and note that $\E[X' \1A] < \infty$, so $\E[Y' \1A] < \infty$.

      By definition of $Z$, this means $\E[Z] \leq 0$. But $Z \geq 0$ almost surely, so $Z=0$ a.s.\ therefore $Y \leq Y'$ a.s.
      (This shows monotonicity of conditional expectation.)
      If $X = X'$, we can run the same argument to show that $Y = Y'$ almost surely (using $A = \{Y > Y'\}$ and $A = \{Y < Y'\}$, we see both sets are measure zero).
    \item Existence. Step 1: Assume first $X \in L^2(\F)$. Since $L^2(\G)$ is a complete subspace of $L^2(\F)$, $X$ has an orthogonal projection $Y$ on $L^2(\G)$, i.e.\ there is $Y \in L^2(\G)$ such that $\E[(X-Y)Z] = 0$ for every $Z \in L^2(\G)$.
      Choosing $Z = \1A$ for $A \in \G$ we get $\E[X \1A] = \E[Y \1A]$ so $Y$ satisfies the conditions of \cref{def:1.2}.

      Step 2: Assume $X \geq 0$. Then $X_n = X \wedge n \in L^2(\F)$ and $0 \leq X_n \nearrow X$ as $n \to \infty$.
      By Step 1, we can find $Y_n \in L^2(\G)$ such that $\E[X_n \1A] = \E[Y_n \1A]$ for all $A \in \G$ and $0 \leq Y_n \leq Y_{n+1}$ almost surely (from the proof of (ii)).
      Let $Y_\infty = \lim_n Y_n \1{\Omega_0}$ with
      \begin{equation*}
        \Omega_0 = \{\omega \in \Omega : 0 \leq Y_n(\omega) \leq Y_{n+1}(\omega) \ \forall n\}.
      \end{equation*}
      Then $Y_\infty$ is a non-negative random variable, is $\G$-measurable as a limit of $\G$-measurable r.v.s and by monotone convergence $\E[X \1A] = \E[Y_\infty \1A]$ for every $A \in \G$.
      Taking $A = \Omega$, $\E[Y_\infty] = \E[X] < \infty$, since $X \in L_1$.
      So $Y_\infty < \infty$ almost surely and $Y \coloneqq Y_\infty \1{\{Y_\infty < \infty\}}$ satisfies \cref{def:1.2}(i) and (ii).

      Step 3: For general $X \in L^1$, apply Step 2 on $X^+$ and $X^-$ to obtain $Y^+$ and $Y^-$. Then $Y = Y^+ - Y^-$ satisfies the conditions of \cref{def:1.2}. \qedhere
  \end{enumerate}
\end{proof}
\begin{eg}[Conditional density functions]
  \newlec
  Let $U$ and $V$ be random variables with a joint density function $f_{U,V}$ in $\mathbb{R}^2$. Then
  \begin{equation*}
    f_U(u) = \int_\mathbb{R} f_{U,V}(u,v) \,dv
  \end{equation*}
  is the density of $U$, and
  \begin{equation*}
    f_{V|U} (v | u) =
    \begin{cases}
      \frac{f_{U,V}(u,v)}{f_U(u)} & \text{if } f_U(u) > 0 \\
      0 & \text{else}
    \end{cases}
  \end{equation*}
  is the conditional density of $V$ given $U$.

  Assume $X = h(V) \in L^1$. Then $\hyperlink{def:eg}{\E[X|U]} = g(U)$ with $g(u) = \int_\mathbb{R} h(v) f_{V|U} (v|u)  \,dv$.
  Indeed, since every $A \in \sigma(U)$ takes the form $A = \{U \in B\}$ for some $B \in \mathcal{B}(\mathbb{R})$.
  \begin{align*}
    \E[X \1A] &= \int_{\mathbb{R}^2} h(v) \1B(u) f_{U,V} (u,v) \, du \, dv \\
              &= \int_\mathbb{R} \underbrace{\left(\int_\mathbb{R} h(v) f_{V|U}(v|u) \,dv\right)}_{g(u)} f_U(u) \1B(u)\,du \\
              &= \E[g(U) \1{\{U \in B\}}] = \E[g(U) \1A].
  \end{align*}
\end{eg}

\subsection{Properties of conditional expectation}
Let $X \in L^1$, $\G \subseteq \F$ $\sigma$-algebras.
\begin{enumerate}[(i)]
  \item $\E[\hyperlink{def:eg}{\E[X | \G]}] = \E[X]$ (proof: use $A = \Omega$ in \cref{def:1.2}(ii))
  \item If $X$ is $\G$-measurable, then $\E[X | \G] = X$ a.s. (proof: $X$ satisfies the conditions of \cref*{def:1.2})
  \item If $X$ is \index{independent}\hypertarget{def:indep}{independent} of $\G$ (i.e. $\Prob[A \cap B] = \Prob[A] \Prob[B]$ for all $A \in \G$ and $B \in \sigma(X)$) then $\E[X | G] = \E[X]$ a.s.
    Proof: $\E[X]$ is constant and thus $\G$-measurable. For $A \in \G$
    \begin{equation*}
      \E[X \1A] = \E[X] \E[\1A] = \E[\E[X] \1A]
    \end{equation*}
    by independence then linearity.
  \item If $X \geq 0$ almost surely then $\E[X|\G] \geq 0$ almost surely.
    (proof: see \cref{thm:1.4}(ii)).
  \item $\E[\alpha X + \beta Y | \G] = \alpha \E[X | \G] + \beta \E[Y | \G]$ almost surely for $Y \in L^1$ and $\alpha,\beta \in \mathbb{R}$.
  \item If $0 \leq X_n \nearrow X$ almost surely, then $\E[X_n | \G] \nearrow \E[X | \G]$ almost surely.
    Proof: $\E[X_n | \G] \nearrow Y$ almost surely for some $\G$-measurable $Y$. For every $A \in \G$,
    \begin{equation*}
      \E[X \1A] = \lim_n \E[X_n \1A] = \lim_n \E[\E[X_n | \G] \1A] = \E[Y \1A]
    \end{equation*}
    so $Y = \E[X | \G]$.
  \item \index{conditional expectation properties!fatou}Fatou. If $X_n \geq 0$ almost surely $\forall n$, then
    \begin{equation*}
      \E[\liminf X_n | \G] \leq \liminf \E[X_n | \G].
    \end{equation*}
    (Proof as for $\E[\;]$).
  \item Dominated convergence. \index{conditional expectation properties!dominated convergence}If $X_n \to X$ almost surely, and $|X_n| \leq Y$ almost surely $\forall n$ for $Y \in L^1$ then $\E[X_n | \G] \to \E[X | \G]$ almost surely. Proof as for $\E[\;]$).
  \item Jensen's inequality. If $c: \mathbb{R} \to (-\infty, \infty]$ convex, then $c(\E[X | \G]) \leq \E[c(X) | \G]$. Proof: $c$ can be written as
    \begin{equation*}
      c(x) = \sup_n (a_n x + b_n) \quad x \in \mathbb{R}
    \end{equation*}
    so
    \begin{equation*}
      \E[c(X) | \G] \geq a_n \E[X | \G] + b_n
    \end{equation*}
    for all $n$.
    Taking $\sup_n$ on the right gives the claim.
  \item $\E[\abs{\E[X|\G]}^p] \leq \E[|X|^p]$ for $1 \leq p < \infty$. Follows from (ix) and (ii).
  \item Tower property: If $\mathcal{H} \subseteq \mathcal{G} \subseteq \mathcal{F}$ $\sigma$-algebras, then
    \begin{equation*}
    \E[\E[X|\G] | \mathcal{H}] = \E[X | \mathcal{H}]
    \end{equation*}
    almost surely.
    Proof: Clearly $\E[\E[X|\G] | \mathcal{H}]$ is $\mathcal{H}$-measurable.
    Take $A \in \mathcal{H}$, so $A \in \G$. Then
    \begin{align*}
      \E[X\1A] &= \E[\1A \E[X|\G]] \\
               &= \E[\1A \E[\E[X|\G] | \mathcal{H}]].
    \end{align*}
  \item Let $Y \in L^1$ be $\G$-measurable, and such that $XY \in L^1$. Then
    \begin{equation*}
      \E[YX | \G] = Y \E[X | G]
    \end{equation*}
    almost surely. `$\G$-measurable random variables behave like constants'.

    Proof: The right hand side of $\G$-measurable. If $Y = \1B$ for $B \in \G$. Then $\forall A \in \G$,
    \begin{equation*}
      \E[XY \1A] = \E[X \1{A\cap B}] = \E[(\E[X | \G] \1B) \1A]
    .\end{equation*}
    So the claim holds for simple random variables. For general $Y$, the statement follows by linearity, approximation, etc.
  \item If $\sigma(X, \G)$ is \hyperlink{def:indep}{independent} of $\mathcal{H}$ then $\E[X | \sigma(\G,\mathcal{H})] = \E[X | \G]$ almost surely.
    Proof: For $A \in \G$ and $B \in \mathcal{H}$,
    \begin{align*}
      \E[\E[X | \sigma(\G,\mathcal{H})] \1{A \cap B}] &= \E[X \1{A \cap B}] \\
                                                      &= \E[X \1{A\cap B}] \\
                                                      &= \E[X \1A] \Prob[B] \\
                                                      &= \E[\E[X | \G] \1A] \Prob[B] \\
                                                      &= \E[\E[X | \G] \1{A \cap B}]\\
      \implies \E[(\E[X | \sigma(\G,\mathcal{H})] - \E[X | \G]) \1{A \cap B}] &= 0
    \end{align*}
    The set of such intersections $A \cap B$ is a \hyperlink{def:pisystem}{$\pi$-system} generating $\sigma(\G, \mathcal{H})$, and it is a standard result of measure theory that this implies $\E[X | \sigma(\G, \mathcal{H})] - \E[X | \G] = 0$ almost surely (see PM notes, Proposition 3.1.4).
\end{enumerate}
\begin{nlemma}\label{lem:1.5}
  \newlec
  Let $X \in L^n$. Then
  \begin{equation*}
    \mathcal{C} = \{Y : Y = \hyperlink{def:eg}{\E[X \mid \G]} \text{ for some } \G \subseteq \F\}
  \end{equation*}
  is uniformly integrable, i.e.\
  \begin{equation*}
    \sup_{Y \in \mathcal{C}} \E[|Y| \1{\{|Y| \geq \lambda\}}] \xrightarrow{\lambda \to \infty} 0
  .\end{equation*}
\end{nlemma}
\begin{proof}
  For every $\epsilon> 0$ there is $\delta > 0$ such that $\E[|X| \1A] \leq \epsilon$ for every $A$ with $\Prob[A] \leq \delta$. (See PM Lemma 6.7.1).

  Choose $\lambda$ such that $\E[|X|] \leq \lambda \delta$.
  For $Y = \E[X \mid \G]$,
  $|Y| \leq \E[|X| \mid \G]$, so $\E[|Y|] \leq \E[\E[|X| \mid \G] ] = \E[|X|] \leq \lambda \delta$.
  Standard bounds give
  \begin{equation*}
    \Prob[|Y| \geq \lambda] \leq \frac{1}{\lambda} \E[|Y|] \leq \delta
  .\end{equation*}
  So
  \begin{align*}
    \E[|Y| \1{\{|Y| \geq \lambda\}}] & \leq \E[\E[|X| \mid \G] \1{\{|Y| \geq \lambda\}}] \\ % this event is in \G?
                                     &= \E[\E[|X| \1{\{|Y| \geq \lambda\}} \mid \G]] \\
                                     &= \E[|X| \1{\{|Y| \geq \lambda\}}] \leq \epsilon
  \end{align*}
  ($\lambda$ independent of $\G$).
\end{proof}

\section{Martingales in discrete time}
\subsection{Definitions}
Let $(\Omega, \F, \Prob)$ a probability space.
\begin{ndef}\hypertarget{def:filtration}{}\hypertarget{def:stoch}{}
  A \named{filtration} is a sequence of $\sigma$-algebras such that
  \begin{equation*}
    \F_n \subseteq \F_{m+n} \subseteq \F\quad \forall n
  .\end{equation*}
  Let $\F_\infty = \sigma(\F_n, n \geq 0)$, so $\F_\infty \subseteq \F$, but possibly $\F_\infty \neq \F$.
  Usually $n$ represents time, and $\F_n$ represents the information available at time $n$.
  A \named{stochastic process} in discrete time $(X_n)_{n \geq 0}$ (i.e. a sequence of random variables) has a \hypertarget{def:natfilt}{\named{natural filtration}} $(\F_n^X)_{n \geq 0}$ given by $\F_n^X = \sigma(X_0, \dotsc, X_n)$.
\end{ndef}
\begin{ndef}\hypertarget{def:adapted}
  A \hyperlink{def:stoch}{stochastic process} $(X_n)_{n \geq 0}$ is \named{adapted} to a \hyperlink{def:filtration}{filtration} $(\F_n)_n$ if $X_n$ is $F_n$-measurable, for all $n$.
\end{ndef}
\begin{ndef}\hypertarget{def:martingale}
  A \hyperlink{def:stoch}{stochastic process} $(X_n)_{n \geq 0}$ is a \named{martingale} (with respect to a \hyperlink{def:filtration}{filtration} $(\F_n)_{n \geq 0}$) if
  \begin{enumerate}[(i)]
    \item $X$ is \hyperlink{def:adapted}{adapted}
    \item $X$ is integrable, i.e.\ each $\E[|X_n|] < \infty$.
    \item $\E[X_{n+1} | \F_n] = X_n$ almost surely (martingale property).
  \end{enumerate}
  If the `$=$' in (iii) is replaced by `$\geq$' (respectively `$\leq$') $X$ is called a \named{submartingale} (resp. \named{supermartingale}).
\end{ndef}
\subsection{Optional stopping}
\begin{ndef}\hypertarget{def:stopping}
  A random time $\tau: \Omega \to \{0, 1, 2, \dotsc\} \cup \{+\infty\}$ is called a \named{stopping time} (with respect to a filtration $\F_n$) if
  \begin{equation*}
    \{\tau \leq n\} \in \F_n \quad \forall n \geq 0
  \end{equation*}
  (or equivalently $\{\tau = n\} \in \F_n \ \forall n$.)
\end{ndef}
\begin{eg}
  Take $X$ \hyperlink{def:adapted}{adapted}, $A \in \mathcal{B}(\mathbb{R})$.
  \begin{equation*}
    \tau_A(\omega) = \inf\{n \geq 0 : X_n(\omega) \in A\}
  \end{equation*}
  the first hitting time of $A$ is a \hyperlink{def:stopping}{stopping time}.
\end{eg}
\hypertarget{def:ftau}We write
\begin{equation*}
  \F_\tau = \{A \in \F_\infty : A \cap \{\tau \leq n\} \in \F_n \ \forall n \geq 0\}.
\end{equation*}
\hypertarget{def:stoppedprocess}{}\hypertarget{def:xtau}Define also $X_\tau(\omega) = X_{\tau(\omega)}(\omega)$ whenever $\tau < \infty$.
The \named{stopped process} $X^\tau$ is given by
\begin{equation*}
  X^\tau_n(\omega) = X_{\tau(\omega) \wedge n}(\omega).
\end{equation*}

\begin{nprop}
  Take $\sigma,\tau$ \hyperlink{def:stopping}{stopping times}, $X$ \hyperlink{def:adapted}{adapted}.
  \begin{enumerate}[(i)]
    \item $\sigma \wedge \tau$ is a \hyperlink{def:stopping}{stopping time}
    \item \hyperlink{def:ftau}{$F_\tau$} is a $\sigma$-algebra
    \item If $\sigma \leq \tau$, $\F_\sigma \subseteq \F_\tau$.
    \item $\hyperlink{def:xtau}{X_\tau} \1{\{\tau < \infty\}}$ is $\F_\tau$-measurable
    \item \hyperlink{def:stoppedprocess}{$X^\tau$} is adapted
    \item If $X$ is integrable, $X^\tau$ is integrable
  \end{enumerate}
\end{nprop}
\begin{nthm}[Hunt's optional stopping theorem]
  Let $X$ be a \hyperlink{def:martingale}{supermartingale}, $\sigma, \tau$ bounded \hyperlink{def:stopping}{stopping times} with $\sigma \leq \tau$. Then
  \begin{equation*}
    \E[X_\tau] \leq \E[X_\sigma]
  .\end{equation*}
\end{nthm}
\begin{proof}
  Let $n \geq 0$ be such that $\tau \leq n$.
  \begin{equation}X_\tau = X_\sigma + \sum_{\sigma \leq k < \tau} (X_{k+1} - X_k) = X_\sigma + \sum_{k=0}^n (X_{k+1} - X_k) \1{\{\sigma \leq k < \tau\}}.\tag{2.1}\label{eq:2.1}\end{equation}
  Now, we have
  \begin{align*}
    \E[(X_{k+1} - X_k) \1{\{\sigma \leq k < \tau\}}] &= \E[\E[(X_{k+1} - X_k) \1{\{\sigma \leq k\}} \1{\{\tau > k\}} | \F_k]] \\
                                                     &= \E[\underbrace{\E[X_{k+1} - X_k | \F_k]}_{\leq 0} \1{\{\sigma \leq k < \tau\}}].
  \end{align*}
  But $X$ is a \hyperlink{def:martingale}{supermartingale}, so taking expectations in \eqref{eq:2.1} gives $\E[X_\tau] \leq \E[X_\sigma]$.
\end{proof}
\begin{remark}
  The same proof shows that if $\sigma,\tau$ are bounded \hyperlink{def:stopping}{stopping times}, $X$ \hyperlink{def:martingale}{submartingale} then $\E[X_\sigma] \leq \E[X_\tau]$.
  Similarly if $X$ is a martingale, we have equality.
\end{remark}
\begin{nthm}
  Let $X$ be \hyperlink{def:adapted}{adapted}, integrable. The following are equivalent.
  \begin{enumerate}[(i)]
    \item $X$ is a \hyperlink{def:martingale}{supermartingale}
    \item For any \hyperlink{def:stopping}{stopping times} $\sigma,\tau$ with $\tau$ bounded, $\E[X_\tau | F_\sigma] \leq X_{\sigma \wedge \tau}$ almost surely.
    \item $X^\tau$ is a supermartingale for all stopping times $\tau$ (not necessarily bounded).
    \item $\E[X_\tau] \leq \E[X_\sigma]$ for all $\sigma,\tau$ stopping times with $\sigma \leq \tau$.
  \end{enumerate}
\end{nthm}
\begin{proof}
  (i) $\implies$ (ii). For $\sigma \geq 0$ and $\tau < n$,
  \begin{align*}
    X_\tau &= X_{\sigma \wedge \tau} + \sum_{\sigma \leq k < \tau} (X_{k+1} - X_k)  \\
           &= X_{\sigma \wedge \tau} + \sum_{k=0}^n (X_{k+1} - X_k) \1{\sigma \leq k < \tau}.
  \end{align*}
  and as in the last proof
  \begin{equation*}
    \E[(X_{k+1} - X_k) \1{\{\sigma \leq k < \tau\}} \1A] \leq 0 \quad \forall A \in F_\sigma
  \end{equation*}
  (note we have $A \cap \{\sigma \leq k\} \in \F_k$).
  Multiplying the earlier equation with $\1A$ and taking expectations gives
  \begin{equation*}
    \E[X_\tau \1A] \leq \E[X_{\sigma \wedge \tau} \1A]
  \end{equation*}
  which implies (ii).

  \newlec
  Clearly (ii) $\Rightarrow$ (iii) and (ii) $\Rightarrow$ (iv) (take expectations).
  Also clearly (iii) $\Rightarrow$ (i), by choosing $\tau$ deterministic and very large.
  Remains to show (iv) $\Rightarrow$ (i).
  Let $m \leq n$, $A \in \F_m$, $\tau \coloneqq m \1A + n \1{A^c} \leq n$.
  \begin{equation*}
    \E[X_n \1A] - \E[X_m \1A] = \E[X_n] - \E[X_n \1{A^c} + X_m \1A]
  .\end{equation*}
  Observe that the rightmost term is just $\E[X_\tau]$, so
  \begin{equation*}
    = \E[X_n] - \E[X_\tau] \leq 0
  \end{equation*}
  by (iv), since $\tau \leq n$.
  Thus $\E[X_n \1A] \leq \E[X_m \1A]\ \forall A \in \F_m$, hence $\E[X_n | \F_m] \leq X_m$ hence $X$ is a supermartingale.
\end{proof}
\subsection{Doob's upcrossing inequality}
Let $X = (X_n)_n$ be a \hyperlink{def:stoch}{stochastic process}. Fix an interval $[a,b]$.
\begin{ndef}\label{def:2.8}\hypertarget{def:upcrossing}
  We say that an \label{upcrossing} of $[a,b]$ occurs between times $m$ and $n$ if
  \begin{enumerate}[(i)]
    \item $X_m < a$ and $X_n > b$
    \item $X_k \in [a,b]$ for every $k \in (m,n)$
  \end{enumerate}
  Write $U_N(a,b) \coloneqq$ number of upcrossings on $[0,N]$
  Define also the monotone limit $U_\infty(a,b) \coloneqq \lim_{N \to \infty} U_N(a,b)$.
\end{ndef}

Consider the gambling strategy: wait until the process $X$ (say price of a share) drops below $a$. Buy a share and hold it until the price exceeds $b$; sell, wait until the price drops below $a$, and so on.

Our wealth process is then given by
\begin{equation*}
  W_n = \sum_{k=1}^n c_k (X_k - X_{k-1})
\end{equation*}
with
\begin{align*}
  c_1 &= \1{\{X_0 < a\}} \\
  c_n &= \1{\{c_{n-1} = 1\}} \1{\{X_{n-1} \leq b\}} + \1{\{c_{n-1} = 0\}} \1{\{X_{n-1} < a\}}
.\end{align*}

Each time there is an \hyperlink{def:upcrossing}{upcrossing}, we win at least $(b-a)$. Thus, at time $N$
\begin{equation}
  W_n \geq (b-a) U_n(a,b) - \underbrace{|a - X_N| \1{\{X_N < a\}}}_{|X_N - a|^-}. \tag{2.3} \label{eq:2.3}
\end{equation}
$|X_N - a|^-$ represents the maximum loss if invested at time $N$ and price $< a$.
\begin{nthm}[Doob's upcrossing lemma]\label{thm:2.9}
  Let $X$ be a \hyperlink{def:martingale}{supermartingale}. Then
  \begin{equation*}
    \E[U_\infty (a,b)] \leq \sup_{n \geq 0} \frac{\E[(X_n - a)^-]}{b-a}
  .\end{equation*}
\end{nthm}
\begin{proof}
  $c_n$ is $F_{m-1}$-measurable and non-negative.
  Hence $(W_n)$ is a \hyperlink{def:martingale}{supermartingale} (easy exercise) with $W_0 = 0$.
  Therefore $\E[W_N] \leq 0$ and taking expectations in \eqref{eq:2.3} gives
  \begin{align*}
    \E[U_N(a,b)] &\leq \frac{\E[(X_N-a)^-]}{b-a} \\
    \E[U_\infty(a,b)] &\leq \sup_{n \geq 0} \frac{\E[(X_n-a)^-]}{b-a}
  \end{align*}
  by monotone convergence.
\end{proof}
\subsection{Doob's maximal inequalities}
Ideal goal: exchange $\E$ and sup.
\begin{equation*}
  X_n^* \coloneqq \sup_{k \leq n} |X_k|
.\end{equation*}
\begin{nthm}[Doob's maximal inequality]\label{thm:2.10}
  Let $X$ be a \hyperlink{def:martingale}{martingale} or non-negative \hyperlink{def:martingale}{submartingale}, then
  \begin{equation*}
    \Prob[X_n^* \geq \lambda] \leq \frac{1}{\lambda} \E[|X_n| \1{\{X_n^* \geq \lambda\}}] \leq \frac{1}{\lambda } \E[|X_n|] \quad \forall \lambda > 0
  .\end{equation*}
\end{nthm}
\begin{proof}
  $X$ \hyperlink{def:martingale}{martingale} $\Rightarrow |X|$ non-negative \hyperlink{def:martingale}{submartingale}. Without loss of generality, assume $X_n \geq 0$ for all $n$.
  Let $\tau \coloneqq \inf\{k \geq 0 : X_k \geq \lambda\} \land n \leq n$, a \hyperlink{def:stopping}{stopping time}.
  \begin{align*}
    \E[X_n] \geq \E[X_\tau] &= \E[\underbrace{X_\tau}_{\geq \lambda} \1{\{X_n^* \geq \lambda\}}] + \E[\underbrace{X_\tau}_{X_n} \1{\{X_n^* < \lambda\}}] \\
    &\geq \lambda \Prob[X_n^* \geq \lambda] + \E[X_n \1{\{X_n^* < \lambda\}}]
  \end{align*}
  hence
  \begin{equation*}
    \lambda \Prob[X_n^* \geq \lambda] \leq \E[X_n \1{\{X_n^* \geq \lambda\}}]
  .\qedhere\end{equation*}
\end{proof}
\begin{nthm}[Doob's $L^p$ inequality]\label{thm:2.11}
  Let $X$ be a \hyperlink{def:martingale}{martingale} or non-negative \hyperlink{def:martingale}{submartingale}. For all $p > 1$,
  \begin{equation*}
    \E[(X_n^*)^p] \leq \left(\frac{p}{p-1}\right)^p \E[|X_n|^p]
  .\end{equation*}
\end{nthm}
\begin{proof}
  Again $X \geq 0$ without loss of generality. Fix $k < \infty$.
  \begin{align*}
    \int_0^k p \lambda^{p-1} \1{\{X_n^* \geq \lambda\}} d\lambda &= p \int_0^{k \land X_n^*} \lambda^{p-1} d\lambda  \\
                                                                 &= (k \land X_n^*)^p
  .\end{align*}
  Then using Fubini and the maximal inequality,
  \begin{align*}
    \E[(k \land X_n^*)^p] &= \int_0^k p \lambda^{p-1} \Prob[X_n^* \geq \lambda] d \lambda \\
                         &\leq \int_0^k p \lambda^{p-2} \E[X_n \1{\{X_n^* \geq \lambda\}}] d \lambda \\
                         &= \frac{p}{p-1} \E\left[X_n \underbrace{\int_0^k (p-1) \lambda^{p-2} \1{\{X_n^* \geq \lambda\}} d \lambda}_{(k \land X_n^*)^{p-1}}\right] \\
                         &= \frac{p}{p-1} \E\left[X_n (k \land X_n^*)^{p-1}\right] \\
                         &\leq \frac{p}{p-1} \E\left[X_n^p\right]^{\frac{1}{p}} \E\left[(k \land X_n^*)^p\right]^{\frac{p-1}{p}} \tag{using H\"older} \\
    \implies \E[(k \land X_n^*)^p]^{\frac{1}{p}} &\leq \frac{p}{p-1} \E[X_n^p]^{\frac{1}{p}}
  \end{align*}
  Finally, use monotone convergence as $k \to \infty$ to give the result.
\end{proof}
Let $X^* = \sup_{n \geq 0} |X_n|$. Taking $n \to \infty$ in \cref{thm:2.10} and \cref{thm:2.11} by monotone convergence, we get
\begin{align*}
  \Prob[X^* > \lambda] &\leq \frac{1}{\lambda} \sup_{n \geq 0} \E[|X_n|] \\
  \E[(X^*)^p] &\leq \left(\frac{p}{p-1}\right) \sup_{n \geq 0} \E[|X_n|^p]
.\end{align*}
\newlec
\subsection{Doob's martingale convergence theorem}
\begin{ndef}\hypertarget{def:lpbounded}
  A stochastic process $(X_n)_{n \geq 0}$ is \index{$L^p$-bounded} if
  \begin{equation*}
    \sup_{n \geq 0} \E[|X_n|^p] < \infty
  .\end{equation*}
  \hypertarget{def:ui}$(X_n)_{n \geq 0}$ is called uniformly integrable (UI) if
  \begin{equation*}
    \sup_{n \geq 0} \E[|X_n| \1{\{|X_n| > \lambda\}}] \xrightarrow{\lambda \to \infty} 0.
  .\end{equation*}
\end{ndef}
\begin{remark}
  $X$ is \hyperlink{def:lpbounded}{$L^p$-bounded} for some $p > 1 \Rightarrow X$ \hyperlink{def:ui}{uniformly integrable}.
  $X$ uniformly integrable $\Rightarrow X$ is $L^1$-bounded.
\end{remark}
\begin{nthm}[Almost sure martingale convergence theorem]\label{thm:2.13}
  Let $(X_n)_{n \geq 0}$ be an \hyperlink{def:lpbounded}{$L^1$-bounded} \hyperlink{def:martingale}{supermartingale}. Then
  \begin{equation*}
    \exists X_\infty \coloneqq \lim_{n \to \infty} X_n
  \end{equation*}
  almost surely, and $X_\infty \in L^1(F_\infty)$.
\end{nthm}
\begin{proof}
  By \nameref{thm:2.9}, for any $a < b$
  \begin{equation*}
  \E[U_\infty(a,b)] \leq \frac{a + \sup{n \geq 0} \E[|X_n|]}{b-a} < \infty
  \end{equation*}
  as $X$ is $L^1$-bounded.
  Then,
  \begin{equation}
    \Prob[U_\infty(a,b) = \infty] =0. \label{eq:2.5} \tag{2.5}
  \end{equation}
  Define
  \begin{align*}
    \Lambda &\coloneqq \{\omega : X_n(\omega) \text{ does not converge to a limit in } [-\infty, \infty]\} \\
            &= \{\omega : \limsup_n X_n(\omega) > \liminf_n X_n(\omega)\} \\
            &= \bigcup_{\substack{a,b \in \mathbb{Q} \\ a<b}} \underbrace{\{\omega : \limsup_n X_n(\omega) > b > a > \liminf_n X_n(\omega)\}}_{\Lambda_{a,b}}
  .\end{align*}
  But $\Lambda_{a,b} \subseteq \{\omega : U_\infty(a,b)(\omega) = \infty\}$, so by \eqref{eq:2.5},
  \begin{align*}
    &\Prob[\Lambda_{a,b}] = 0  \quad \forall a,b
    \implies &\Prob[\bigcup_{\substack{a,b \in \mathbb{Q} \\ a < b}} \Lambda_{a,b}] = 0 \\
    \implies &\Prob[\Lambda] = 0
  .\end{align*}
  hence $X = \lim_{n \to \infty} X_n \in [-\infty, \infty]$ exists almost surely.
  Now,
  \begin{equation*}
    \E[|X_\infty|] = \E[\liminf_n |X_n|] \leq \liminf \E[|X_n|] \leq \sup_{n \geq 0} \E[|X_n|] < \infty
  \end{equation*}
  hence $X_\infty \in L^1$.
\end{proof}
\begin{remark}
  \cref{thm:2.13} implies that non-negative supermartingales always converge almost surely, because then $\E[|X_n|] = \E[X_n] \leq \E[X_0] < \infty$, so $X$ is $L^1$-bounded.
\end{remark}
\begin{nthm}[$L^1$ martingale convergence]\label{thm:2.14}
  \begin{enumerate}[(i)]
    \item Let $(X_n)$ be a \hyperlink{def:ui}{uniformly integrable} \hyperlink{def:martingale}{martingale}. Then
      $\exists X_\infty \in L^1(\F_\infty)$ such that $X_n \to X_\infty$ almost surely and in $L^1$ and $X_n = \E[X_\infty | \F_n]$ almost surely for every $n$.
    \item For any $Y \in L^1(\F_\infty)$, $X_n = \E[Y | \F_n]$ defines a uniformly integrable martingale with $X_n \to Y$ almost surely and in $L^1$.
  \end{enumerate}
\end{nthm}
\begin{proof}
  \begin{enumerate}[(i)]
    \item By \cref{thm:2.13}, $X_n \to X_\infty \in L^1(\F_\infty)$ almost surely, so because $X$ is \hyperlink{def:ui}{uniformly integrable}, $X_n \to X_\infty$ in $L^1$.
      For $m \geq n$,
      \begin{align*}
        \E[|X_n - \E[X_\infty | \F_n]|] &= \E[|\E[X_m - X_\infty | \F_n] |] \\
                                        &\leq \E[|X_m - X_\infty|] \xrightarrow {m \to \infty} 0 \\
        \implies X_n &= \E[X_\infty | \F_n]
      .\end{align*}
    \item $(X_n)$ is a martingale and \hyperlink{def:ui}{uniformly integrable} by \cref{lem:1.5}.
      Hence $\exists X_\infty \in L^1(\F_\infty)$ with $X_n \to X_\infty$ almost surely and in $L^1$.
      \begin{equation*}
        \E[X_\infty \1A] = \lim_n \E[X_n \1A] = \E[Y \1A]
      \end{equation*}
      for each $A \in \F_n$ for $n \geq 0$.
      $\bigcup \F_n$ is a $\pi$-system which generates $F_\infty$, so $X_\infty = Y$ almost surely. \qedhere
  \end{enumerate}
\end{proof}
\begin{nthm}[$L^p$ martingale convergence theorem]\label{thm:2.15}\leavevmode
  \begin{enumerate}[(i)]
    \item Let $(X_n)$ be an \hyperlink{def:lpbounded}{$L^p$-bounded} \hyperlink{def:martingale}{martingale}, with $p > 1$.
      Then $\exists X_\infty \in L^p(\F_\infty)$ such that
      $X_n \to X_\infty$ \hyperlink{def:as}{almost surely} and in $L^p$ and $X_n = \E[X_\infty | \F_n]$ almost surely for all $n$.
    \item For any $Y \in L^p(\F_\infty)$, $X_n = \E[Y | \F_n]$ defines an $L^p$-bounded martingale with $X_n \to Y$ almost surely and in $L^p$.
  \end{enumerate}
\end{nthm}
\begin{proof}
  \begin{enumerate}[(i)]
    \item Again by \cref{thm:2.13}, $X_n \to X_\infty$ almost surely, and $X$ is $L^p$ bounded.
      Doob's maximal inequality % TODO ref
      gives
      \begin{equation*}
        \E[(X^*)^p)] \leq (\frac{p}{p-1})^p \sup_{n \geq 0} \E[|X_n|^p] < \infty.
      .\end{equation*}
      Recalling $X^* = \sup_{n \geq 0} |X_n|$, we have $|X_n - X_\infty|^p \leq (2 X^*)^p$, hence $X_n \to X_\infty$ in $L^p$ by dominated convergence.
      $X_n = \E[X_\infty | \F_n]$, as in the $L^1$ case.
    \item $(X_n)$ is a martingale and
      \begin{equation*}
        \E[|X_n|^p] = \E[|\E[Y | \F_n]|^p] \leq \E[|Y|^p] < \infty
      \end{equation*}
      hence $(X_n)$ is an $L^p$-bounded martingale, so by (i), $X_n \to X_\infty$ almost surely and in $L^p$ for some $X_\infty \in L^p$.
      Identify $X_\infty = Y$ as in the $L^1$-case.
  \end{enumerate}
\end{proof}
Let $(\hat{\F}_n)$ be a backward filtration, i.e.\ a sequence of $\sigma$-algebras such that
\begin{equation*}
  \F \supseteq \hat{\F}_n \supseteq \hat{\F}_{n+1} \quad \forall n \geq 0.
.\end{equation*}
Let $\hat{\F}_\infty = \bigcup_{n \geq 0} \hat{F}_n$. (Note the intersection of $\sigma$-algebras is a $\sigma$-algebra, so we don't need to add the $\sigma$ like for a usual \hyperlink{def:filtration}{filtration}.)
\begin{nthm}[Backward martingale convergence]\label{thm:2.16}
  For all $Y \in L^1(\F)$,
  \begin{equation*}
    \E[Y | \hat{\F}_n] \xrightarrow{n \to \infty} \E[Y | \hat{\F}_\infty]
  \end{equation*}
  almost surely and in $L^1$.
\end{nthm}
\begin{proof}
  Let $X_n = \E[Y | \hat{\F}_n]$. For fixed $n \geq 0$, $(X_{n-k})_{0 \leq k \leq n}$ is a martingale with respect to $(\hat{\F}_{n-k})_{0 \leq k \leq n}$.
\end{proof}
\printindex
\end{document}
